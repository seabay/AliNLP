{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "OoFMDGokWQWf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZM9y8u6LC1UA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "import math, copy, time\n",
        "from collections import Counter, OrderedDict\n",
        "import re\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import PackedSequence,pack_padded_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "MIN_LENGTH = 3\n",
        "MAX_LENGTH = 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M1HkJl0sWLQR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2364b2a-e0ca-4e7f-9bc0-13dd268b179f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522398298,
          "user_tz": 240,
          "elapsed": 331,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
        "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\n",
        "USE_CUDA"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "ZO3waJiiNaMq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8e48e01-6764-477c-f421-5e288937a804",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522398940,
          "user_tz": 240,
          "elapsed": 498,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1111)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8d1b321fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "KJf6TPffNbfs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# 安装 PyDrive 操作库，该操作每个 notebook 只需要执行一次\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 授权登录，仅第一次的时候会鉴权\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8DfzgqwINpTZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c3d7ce8a-ec62-420c-e422-a1faa8495007",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522405954,
          "user_tz": 240,
          "elapsed": 1071,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# 列出根目录的所有文件\n",
        "# \"q\" 查询条件教程详见：https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s, mimeType: %s' % (file1['title'], file1['id'], file1[\"mimeType\"]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: data, id: 16zfnt8Cnuznzwj2RHGrxvyfyA6iQPRXx, mimeType: application/vnd.google-apps.folder\n",
            "title: Colab Notebooks, id: 1DDn16_H4dL75WfkYDpWNxRzqJkH6uvsk, mimeType: application/vnd.google-apps.folder\n",
            "title: 02 - email-templates.png, id: 0BzICsNHMK-GFdnZhNFBoVXdobkhBa3h2eUtwbXY1ek15Sk53, mimeType: image/png\n",
            "title: ATEC-Alipay, id: 1UyfXNCoPHh73E4jO-pgombXQShfYkzAVgGfSUnU7OAE, mimeType: application/vnd.google-apps.document\n",
            "title: The Annotated _Attention is All You Need_.ipynb, id: 1zIkaTyyRtMY0qRTuUxe9J10xR3S9J5Xc, mimeType: application/vnd.google.colab\n",
            "title: Copy of mixture_of_softmaxes.ipynb, id: 1HiJvBimCdWvFP7SoLs9bWNR-JbCQSRsG, mimeType: application/vnd.google.colab\n",
            "title: Copy of Hello, Colaboratory, id: 0BzICsNHMK-GFcFBrQkV2UXpMVGM, mimeType: application/vnd.google.colab\n",
            "title: 2017 Calendar, id: 1jw6VChc9SGayYuK5Zmy2PjaiwWM0tzGFrPG9eatsSNw, mimeType: application/vnd.google-apps.spreadsheet\n",
            "title: Getting started, id: 0BzICsNHMK-GFc3RhcnRlcl9maWxlX2Rhc2hlclYw, mimeType: application/pdf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BUjmtQCvNtdg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a079b61-0002-4743-b44f-2bb6ba3e07e8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522406696,
          "user_tz": 240,
          "elapsed": 510,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# '目录 id' in parents\n",
        "file_list = drive.ListFile({'q': \"'16zfnt8Cnuznzwj2RHGrxvyfyA6iQPRXx' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s, mimeType: %s' % (file1['title'], file1['id'], file1[\"mimeType\"]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: atec_nlp_sim_train.csv, id: 1py3JAFOLOH2lM-lOqYyP_LfqaImlOqL_, mimeType: text/csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BGJObD-jOOHe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "file = drive.CreateFile({'id': \"1py3JAFOLOH2lM-lOqYyP_LfqaImlOqL_\"}) \n",
        "#这里的下载操作只是缓存，不会在你的Google Drive 目录下多下载一个文件\n",
        "file.GetContentFile('data.csv', \"text/csv\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Mb6y3drORY6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data.csv', sep='\\t', names=['number', \"sen1\", \"sen2\", \"label\"], skipinitialspace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oAj4-3-pObOT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def normalize_string(s):\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    s = re.sub(r\"\\d+\", r\"N\", s).strip()\n",
        "    return s\n",
        "  \n",
        "  \n",
        "flatten = lambda l: [item for sublist in l for item in sublist]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kwnvx9MfOi_H",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ddcafdf7-73f5-462a-be48-f6120e9bac44",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522410866,
          "user_tz": 240,
          "elapsed": 827,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "X1_r = list(map(normalize_string, df.sen1.tolist()))\n",
        "X2_r = list(map(normalize_string, df.sen2.tolist()))\n",
        "y_r = df.label.tolist()\n",
        "\n",
        "for sen1, sen2, y in zip(X1_r, X2_r, y_r):\n",
        "  if len(sen1) > MAX_LENGTH or len(sen2) > MAX_LENGTH:\n",
        "    X1_r.remove(sen1)\n",
        "    X2_r.remove(sen2)\n",
        "    y_r.remove(y)\n",
        "    \n",
        "    X1_r.append(sen1[:MAX_LENGTH])\n",
        "    X2_r.append(sen2[:MAX_LENGTH])\n",
        "    y_r.append(y)\n",
        "    \n",
        "print(len(X1_r), len(X2_r), len(y_r))\n",
        "print(X1_r[0], \"@@@@\", X2_r[0], \"@@@@\",y_r[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39346 39346 39346\n",
            "﻿怎么更改花呗手机号码 @@@@ 我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号 @@@@ 1\n",
            "CPU times: user 484 ms, sys: 0 ns, total: 484 ms\n",
            "Wall time: 492 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wWANOepMOlld",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d95984d1-48b5-4e58-cde6-c146c0957ce7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522411439,
          "user_tz": 240,
          "elapsed": 416,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = list(set(flatten(X1_r + X2_r)))\n",
        "len(vocab)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1701"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "iJpiz1ATOv8l",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60c9039e-9020-4636-c2f2-1c236e328eac",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522411881,
          "user_tz": 240,
          "elapsed": 279,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "source2index = {'<PAD>':0,'<UNK>':1,'<s>':2,'</s>':3}\n",
        "for vo in vocab:\n",
        "    if vo not in source2index.keys():\n",
        "        source2index[vo]=len(source2index)\n",
        "index2source = {v:k for k,v in source2index.items()}\n",
        "\n",
        "vocab = source2index.keys()\n",
        "len(vocab)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1705"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "1Y7ZngnFO25U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def getBatch(batch_size,train_data):\n",
        "    random.shuffle(train_data)\n",
        "    sindex=0\n",
        "    eindex=batch_size\n",
        "    while eindex < len(train_data):\n",
        "        batch = train_data[sindex:eindex]\n",
        "        temp = eindex\n",
        "        eindex = eindex+batch_size\n",
        "        sindex = temp\n",
        "        yield batch\n",
        "    \n",
        "    if eindex >= len(train_data):\n",
        "        batch = train_data[sindex:]\n",
        "        yield batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O5rt9Y10O5Zj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_index):\n",
        "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<unk>\"], seq))\n",
        "    return LongTensor(idxs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JgbUEaoVO7vq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fc0028d4-6a7b-4ecb-9cb3-2c7c8dfa35e7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522413823,
          "user_tz": 240,
          "elapsed": 1029,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X1_p, X2_p = [],[]\n",
        "ta_p = []\n",
        "\n",
        "for s1, s2, ta in zip(X1_r, X2_r, y_r):\n",
        "    X1_p.append(prepare_sequence(s1,source2index).view(1,-1))\n",
        "    X2_p.append(prepare_sequence(s2,source2index).view(1,-1))\n",
        "    ta_p.append(ta)\n",
        "    \n",
        "data = list(zip(X1_p, X2_p, ta_p))\n",
        "np.random.shuffle(data)\n",
        "train_data = data[:30000]\n",
        "validate_data = data[30000:35000]\n",
        "test_data = data[35000:]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 720 ms, sys: 13 ms, total: 733 ms\n",
            "Wall time: 734 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7coYOFhRPaxz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a00b3014-fac8-4351-e037-21b491a17363",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522414134,
          "user_tz": 240,
          "elapsed": 273,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "np.sum(np.array([i[2] for i in validate_data]) == 1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1087"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "8EWNXQ1qPbyF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "053656ce-2d61-4d13-a036-314571ec3781",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522414627,
          "user_tz": 240,
          "elapsed": 378,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "np.sum(np.array([i[2] for i in validate_data]) == 0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3913"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "sb2LrYZLO-22",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def pad_to_batch(batch,x_to_ix):\n",
        "    \n",
        "    sorted_batch =  sorted(batch, key=lambda b:b[0].size(1),reverse=True) # sort by len\n",
        "    \n",
        "    x1, x2, y = list(zip(*sorted_batch))\n",
        "    max_x1 = max([s.size(1) for s in x1])\n",
        "    max_x2 = max([s.size(1) for s in x2])\n",
        "    \n",
        "    x1_p, x2_p, y_p = [],[],[]\n",
        "    for i in range(len(batch)):\n",
        "      \n",
        "        if x1[i].size(1)<max_x1:\n",
        "            x1_p.append(torch.cat( [ Variable(x1[i]),Variable(LongTensor([x_to_ix['<PAD>']]*(max_x1-x1[i].size(1)))).view(1,-1) ],1))\n",
        "        else:\n",
        "            x1_p.append(Variable(x1[i]))\n",
        "        \n",
        "        if x2[i].size(1)<max_x2:\n",
        "            x2_p.append(torch.cat( [ Variable(x2[i]),Variable(LongTensor([x_to_ix['<PAD>']]*(max_x2-x2[i].size(1)))).view(1,-1) ],1))\n",
        "        else:\n",
        "            x2_p.append(Variable(x2[i]))\n",
        "        \n",
        "    x1_var = torch.cat(x1_p, 0)\n",
        "    x2_var = torch.cat(x2_p, 0)\n",
        "    target_var = Variable(LongTensor(y))\n",
        "    x1_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in x1_var]\n",
        "    x2_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in x2_var]\n",
        "    \n",
        "    return x1_var, x2_var, target_var, x1_len, x2_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qnNEua43PNyv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "EPOCH=150\n",
        "BATCH_SIZE = 128\n",
        "EMBEDDING_SIZE = 128\n",
        "LR = 0.01\n",
        "DECODER_LEARNING_RATIO=5.0\n",
        "RESCHEDULED=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mLo7GMCnPB12",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "2ebe290f-eeb1-4a2f-eef1-46263543f9dd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522415885,
          "user_tz": 240,
          "elapsed": 324,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch = next(getBatch(BATCH_SIZE,train_data))\n",
        "pbatch = pad_to_batch(batch,source2index)\n",
        "pbatch[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable containing:\n",
              "  830  1447  1212  ...   1212  1152   611\n",
              " 1497   803   405  ...    398     0     0\n",
              "  217   836  1683  ...      0     0     0\n",
              "       ...          ⋱          ...       \n",
              "  405   442   825  ...      0     0     0\n",
              "  405   442  1034  ...      0     0     0\n",
              "  405   442  1346  ...      0     0     0\n",
              "[torch.LongTensor of size 128x40]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "vPaFUK3OPVK3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_perf():\n",
        "  \n",
        "  print(\"========== performance on randomly selected 5000 samples from training data\")\n",
        "  samples = random.sample(train_data, 5000)\n",
        "  sen1, sen2, targets,sen1_lengths, sen2_lengths = pad_to_batch(samples,source2index)\n",
        "  outputs = model(sen1, sen2, sen1_lengths, sen2_lengths)\n",
        "  t_predict = np.array(np.argmax(outputs.data, 1))\n",
        "  t_targets = np.array(targets.data)\n",
        "  print(\"Correct: \", np.sum(t_targets == t_predict))\n",
        "  print(\"tn, fp, fn, tp \\n\", confusion_matrix(t_targets, t_predict).ravel())\n",
        "  print(\"precsion: \", precision_score(t_predict, t_targets))\n",
        "  print(\"recall: \", recall_score(t_predict, t_targets))\n",
        "  print(\"f1 score: \", f1_score(t_predict, t_targets))\n",
        "  \n",
        "def validate_perf():\n",
        "  \n",
        "  print(\"========== performance on validation data\")\n",
        "  sen1, sen2, targets,sen1_lengths, sen2_lengths = pad_to_batch(validate_data,source2index)\n",
        "  outputs = model(sen1, sen2, sen1_lengths, sen2_lengths)\n",
        "  predict = np.array(np.argmax(outputs.data, 1))\n",
        "  targets = np.array(targets.data)\n",
        "  print(\"Correct: \", np.sum(predict == targets))\n",
        "  print(\"tn, fp, fn, tp \\n\", confusion_matrix(targets, predict).ravel())\n",
        "  print(\"precsion: \", precision_score(predict, targets))\n",
        "  print(\"recall: \", recall_score(predict, targets))\n",
        "  print(\"f1 score: \", f1_score(predict, targets))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vW5ZtaGaDXt0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_AsAEcVFCQ2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtFpgAsiFEqt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YcHgnDIGEYzN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IQpSxGRpEbdn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        #return x + self.dropout(sublayer(self.norm(x)))  # this is not LayerNorm(x+Sublayer(x))\n",
        "        return self.norm(x + self.dropout(sublayer(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YVVzrT8UE2iE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ju44Z6CbE_x_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZHsGDKUE8vG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, hidden_size, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert hidden_size % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = hidden_size // h         # we employ h=4 parallel attention layers, 8//4=2\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(hidden_size, hidden_size), 4) #2nd d_model should be dk or dv, WQ, WK, WV, WO\n",
        "        #self.output = nn.Linear(hidden_size * h, hidden_size)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        #print(\"nbatches\", nbatches)\n",
        "        \n",
        "        #print(\"query, key, value - 1\", query, key, value)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k = d_model(hidden_size), hidden 被拆分为3部分\n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        #print(\"query, key, value - 2\", query, key, value)\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3XNby5SaEe47",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))  ## k, q, v = x, x, x\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tKGd7CR9Da_-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N, vocab_size, hidden_size, position):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Sequential(Embeddings(hidden_size, vocab_size), position)\n",
        "        \n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        x = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-kJNvYJxP9PX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, kernel_num=100, kernel_sizes=[3,4,5], dropout=0.1):\n",
        "        \n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_num, (K, EMBEDDING_SIZE)) for K in kernel_sizes])\n",
        "\n",
        "        # kernal_size = (K,D) \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(len(kernel_sizes)*kernel_num, 2)\n",
        "        \n",
        "    def init_weight(self):\n",
        "        pass\n",
        "        \n",
        "    def forward(self, sen1, sen2, sen1_lengths, sen2_lengths, is_training=False):\n",
        "             \n",
        "        outputs_1 = self.encoder(sen1,None)\n",
        "        outputs_2 = self.encoder(sen2,None)\n",
        "        \n",
        "        #print(\"outputs_1: \", outputs_1.shape)\n",
        "        #print(\"outputs_2: \", outputs_2.shape)\n",
        "        inputs = torch.cat((outputs_1, outputs_2), 1)\n",
        "        #print(\"inputs: \", inputs.shape)\n",
        "        \n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        #print(\"inputs: \", inputs.shape)\n",
        "        \n",
        "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
        "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
        "\n",
        "        concated = torch.cat(inputs, 1)\n",
        "\n",
        "        if is_training:\n",
        "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
        "        out = self.fc(concated) \n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c-feqH0_FaB5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def make_model(vocab_size, N=2, \n",
        "               hidden_size=8, d_ff=4, h=2, dropout=0.1):\n",
        "  \n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, hidden_size)\n",
        "    ff = PositionwiseFeedForward(hidden_size, d_ff, dropout)\n",
        "    position = PositionalEncoding(hidden_size, dropout)\n",
        "    \n",
        "    el = EncoderLayer(hidden_size, c(attn), c(ff), dropout)\n",
        "    encoder = Encoder(el, N, vocab_size, hidden_size, c(position))\n",
        "    model = Model(encoder)\n",
        "    model.init_weight()\n",
        "\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dc58tjA2lRLu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "alpha = Variable(FloatTensor([0.75, 0.25]))  # increase loss weights for positive samples\n",
        "gamma = 2\n",
        "\n",
        "def one_hot(index, classes):\n",
        "    size = index.size() + (classes,)\n",
        "    view = index.size() + (1,)\n",
        "\n",
        "    mask = FloatTensor(*size).fill_(0)\n",
        "    index = index.view(*view)\n",
        "    ones = 1.\n",
        "\n",
        "    if isinstance(index, Variable):\n",
        "        ones = Variable(FloatTensor(index.size()).fill_(1))\n",
        "        mask = Variable(mask, volatile=index.volatile)\n",
        "\n",
        "    return mask.scatter_(1, index, ones)\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, gamma=0, eps=1e-7):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        y = one_hot(target, input.size(-1))\n",
        "        logit = F.softmax(input, dim=-1)\n",
        "        logit = logit.clamp(self.eps, 1. - self.eps)   # pt\n",
        "\n",
        "        loss = -1 * y * torch.log(logit) * alpha # cross entropy\n",
        "        loss = loss * (1 - logit) ** self.gamma # focal loss\n",
        "\n",
        "        return loss.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orCuaOMaHBiL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Small example model.\n",
        "EMBEDDING_SIZE = 64\n",
        "LR = 0.01\n",
        "\n",
        "model = make_model(len(vocab), 2, EMBEDDING_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Qk4MJEs2G9I",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1360
        },
        "outputId": "8437990e-13b8-46b4-b513-c8f0a088d16a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522421866,
          "user_tz": 240,
          "elapsed": 242,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.parameters"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Model(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Sequential(\n",
              "      (0): Embeddings(\n",
              "        (lut): Embedding(1705, 64)\n",
              "      )\n",
              "      (1): PositionalEncoding(\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=64, out_features=64)\n",
              "            (1): Linear(in_features=64, out_features=64)\n",
              "            (2): Linear(in_features=64, out_features=64)\n",
              "            (3): Linear(in_features=64, out_features=64)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=64, out_features=4)\n",
              "          (w_2): Linear(in_features=4, out_features=64)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=64, out_features=64)\n",
              "            (1): Linear(in_features=64, out_features=64)\n",
              "            (2): Linear(in_features=64, out_features=64)\n",
              "            (3): Linear(in_features=64, out_features=64)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=64, out_features=4)\n",
              "          (w_2): Linear(in_features=4, out_features=64)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm(\n",
              "    )\n",
              "  )\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d (1, 100, kernel_size=(3, 64), stride=(1, 1))\n",
              "    (1): Conv2d (1, 100, kernel_size=(4, 64), stride=(1, 1))\n",
              "    (2): Conv2d (1, 100, kernel_size=(5, 64), stride=(1, 1))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1)\n",
              "  (fc): Linear(in_features=300, out_features=2)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "lBMgoVbvmJCG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = FocalLoss(2)\n",
        "optimizer = optim.Adam(model.parameters(),lr=LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r_yviALlL4qn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "8a191751-4afc-4531-d74a-1bbc78678821",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526522590454,
          "user_tz": 240,
          "elapsed": 34389,
          "user": {
            "displayName": "Pengcheng Jia",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112854971302097969837"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# batch * length * embedding_size\n",
        "\n",
        "for i,batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
        "    \n",
        "    # sen1 = batch * l\n",
        "    sen1, sen2, targets,sen1_lengths, sen2_lengths = pad_to_batch(batch,source2index)\n",
        "    print(\"sen1 size: \", sen1.shape)\n",
        "    \n",
        "    # Forward + Backward + Optimize\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(sen1, sen2, sen1_lengths, sen2_lengths, True)\n",
        "    print('outputs', outputs.shape)\n",
        "    print('targets', targets.shape)\n",
        "    loss = criterion(outputs, targets)\n",
        "    print('loss', loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    break\n",
        "    \n",
        "train_perf()\n",
        "validate_perf()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sen1 size:  torch.Size([128, 33])\n",
            "outputs torch.Size([128, 2])\n",
            "targets torch.Size([128])\n",
            "loss Variable containing:\n",
            " 100.7381\n",
            "[torch.FloatTensor of size 1]\n",
            "\n",
            "========== performance on randomly selected 5000 samples from training data\n",
            "Correct:  3892\n",
            "tn, fp, fn, tp \n",
            " [3892    0 1108    0]\n",
            "precsion:  0.0\n",
            "recall:  0.0\n",
            "f1 score:  0.0\n",
            "========== performance on validation data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Correct:  3913\n",
            "tn, fp, fn, tp \n",
            " [3913    0 1087    0]\n",
            "precsion:  0.0\n",
            "recall:  0.0\n",
            "f1 score:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tzqOvhl2wFB2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "3ae1769b-ae31-483a-8a8e-47824dd140bc"
      },
      "cell_type": "code",
      "source": [
        "total_step = round(len(train_data) / BATCH_SIZE)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    \n",
        "    total_loss = []\n",
        "    \n",
        "    for i,batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
        "    \n",
        "      # sen1 = batch * l\n",
        "      sen1, sen2, targets,sen1_lengths, sen2_lengths = pad_to_batch(batch,source2index)\n",
        "\n",
        "      # Forward + Backward + Optimize\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(sen1, sen2, sen1_lengths, sen2_lengths, True)\n",
        "      loss = criterion(outputs, targets)\n",
        "      total_loss.append(loss.data[0])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (i+1) % 20 == 0:\n",
        "          print ('Epoch [{}/{}], Step [{}/{}], Mean Loss: {:.4f}' \n",
        "                     .format(epoch+1, EPOCH, i+1, total_step, np.mean(total_loss)))\n",
        "          total_loss = []\n",
        "    \n",
        "    if (epoch + 1) %10 == 0:\n",
        "        train_perf()\n",
        "        validate_perf()\n",
        "    \n",
        "    #if RESCHEDULED == False and epoch  == EPOCH//4:\n",
        "    if (epoch + 1) %25 == 0:\n",
        "        LR *= 0.1\n",
        "        print(\"LR now is: \", LR)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/150], Step [20/234], Mean Loss: 115.6473\n",
            "Epoch [1/150], Step [40/234], Mean Loss: 116.2517\n",
            "Epoch [1/150], Step [60/234], Mean Loss: 115.4458\n",
            "Epoch [1/150], Step [80/234], Mean Loss: 113.0281\n",
            "Epoch [1/150], Step [100/234], Mean Loss: 110.6104\n",
            "Epoch [1/150], Step [120/234], Mean Loss: 111.8193\n",
            "Epoch [1/150], Step [140/234], Mean Loss: 114.6399\n",
            "Epoch [1/150], Step [160/234], Mean Loss: 107.9912\n",
            "Epoch [1/150], Step [180/234], Mean Loss: 111.0134\n",
            "Epoch [1/150], Step [200/234], Mean Loss: 110.4089\n",
            "Epoch [1/150], Step [220/234], Mean Loss: 106.5809\n",
            "Epoch [2/150], Step [20/234], Mean Loss: 113.6325\n",
            "Epoch [2/150], Step [40/234], Mean Loss: 114.6399\n",
            "Epoch [2/150], Step [60/234], Mean Loss: 112.2222\n",
            "Epoch [2/150], Step [80/234], Mean Loss: 113.2296\n",
            "Epoch [2/150], Step [100/234], Mean Loss: 111.0134\n",
            "Epoch [2/150], Step [120/234], Mean Loss: 111.4163\n",
            "Epoch [2/150], Step [140/234], Mean Loss: 111.0134\n",
            "Epoch [2/150], Step [160/234], Mean Loss: 111.8193\n",
            "Epoch [2/150], Step [180/234], Mean Loss: 116.8562\n",
            "Epoch [2/150], Step [200/234], Mean Loss: 112.6252\n",
            "Epoch [2/150], Step [220/234], Mean Loss: 110.6104\n",
            "Epoch [3/150], Step [20/234], Mean Loss: 113.8340\n",
            "Epoch [3/150], Step [40/234], Mean Loss: 113.8340\n",
            "Epoch [3/150], Step [60/234], Mean Loss: 120.2813\n",
            "Epoch [3/150], Step [80/234], Mean Loss: 102.3499\n",
            "Epoch [3/150], Step [100/234], Mean Loss: 114.8414\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}