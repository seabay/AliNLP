{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<unk>\"], seq))\n",
    "    return LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_batch(batch,x_to_ix):\n",
    "    \n",
    "    sorted_batch =  sorted(batch, key=lambda b:b[0].size(1),reverse=True) # sort by len\n",
    "    x,y = list(zip(*sorted_batch))\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    \n",
    "    x_p,y_p=[],[]\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1)<max_x:\n",
    "            x_p.append(torch.cat([x[i],Variable(LongTensor([x_to_ix['<PAD>']]*(max_x-x[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "        \n",
    "        \n",
    "    input_var = torch.cat(x_p)\n",
    "    target_var = torch.cat(y)\n",
    "    input_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in input_var]\n",
    "    target_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in target_var]\n",
    "    \n",
    "    return input_var, target_var, input_len, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/atec_nlp_sim_train.csv', sep='\\t', names=['number', \"sen1\", \"sen2\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X1_r = list(map(normalize_string, df.sen1.tolist()))\n",
    "X2_r = list(map(normalize_string, df.sen2.tolist()))\n",
    "y_r = df.label.tolist()\n",
    "print(len(X1_r), len(X2_r), len(y_r))\n",
    "print(X1_r[0], \"@@@@\", X2_r[0], \"@@@@\",y_r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(X1_r + X2_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source2index = {'<PAD>':0,'<UNK>':1,'<s>':2,'</s>':3}\n",
    "for vo in vocab:\n",
    "    if vo not in source2index.keys():\n",
    "        source2index[vo]=len(source2index)\n",
    "index2source = {v:k for k,v in source2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X1_p, X2_p = [],[]\n",
    "ta_p = []\n",
    "\n",
    "for s1, s2, ta in zip(X1_r, X2_r, y_r):\n",
    "    X1_p.append(prepare_sequence(s1,source2index).view(1,-1))\n",
    "    X2_p.append(prepare_sequence(s2,source2index).view(1,-1))\n",
    "    ta_p.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(X1_p, X2_p, ta_p))\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_GRAM_SIZE = 3 # See section 3.2.\n",
    "WINDOW_SIZE = 3 # See section 3.2.\n",
    "TOTAL_LETTER_GRAMS = int(3 * 1e4) # Determined from data. See section 3.2.\n",
    "WORD_DEPTH = WINDOW_SIZE * TOTAL_LETTER_GRAMS # See equation (1).\n",
    "# Uncomment it, if testing\n",
    "# WORD_DEPTH = 1000\n",
    "K = 300 # Dimensionality of the max-pooling layer. See section 3.4.\n",
    "L = 128 # Dimensionality of latent semantic space. See section 3.5.\n",
    "J = 4 # Number of random unclicked documents serving as negative examples for a query. See section 4.\n",
    "FILTER_LENGTH = 1 # We only consider one time step for convolutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmax_pooling(x, dim, k):\n",
    "    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "    return x.gather(dim, index)\n",
    "\n",
    "class CDSSM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CDSSM, self).__init__()\n",
    "        # layers for query\n",
    "        self.query_conv = nn.Conv1d(WORD_DEPTH, K, FILTER_LENGTH)\n",
    "        self.query_sem = nn.Linear(K, L)\n",
    "        # layers for docs\n",
    "        self.doc_conv = nn.Conv1d(WORD_DEPTH, K, FILTER_LENGTH)\n",
    "        self.doc_sem = nn.Linear(K, L)\n",
    "        # learning gamma\n",
    "        self.learn_gamma = nn.Conv1d(1, 1, 1)\n",
    "        \n",
    "        self.fc = nn.Linear(2*L, L)\n",
    "        \n",
    "    def forward(self, q, pos, negs):\n",
    "        # Query model. The paper uses separate neural nets for queries and documents (see section 5.2).\n",
    "        # To make it compatible with Conv layer we reshape it to: (batch_size, WORD_DEPTH, query_len)\n",
    "        q = q.transpose(1,2)\n",
    "        # In this step, we transform each word vector with WORD_DEPTH dimensions into its\n",
    "        # convolved representation with K dimensions. K is the number of kernels/filters\n",
    "        # being used in the operation. Essentially, the operation is taking the dot product\n",
    "        # of a single weight matrix (W_c) with each of the word vectors (l_t) from the\n",
    "        # query matrix (l_Q), adding a bias vector (b_c), and then applying the tanh activation.\n",
    "        # That is, h_Q = tanh(W_c • l_Q + b_c). Note: the paper does not include bias units.\n",
    "        q_c = F.tanh(self.query_conv(q))\n",
    "        # Next, we apply a max-pooling layer to the convolved query matrix.\n",
    "        q_k = kmax_pooling(q_c, 2, 1)\n",
    "        q_k = q_k.transpose(1,2)\n",
    "        # In this step, we generate the semantic vector represenation of the query. This\n",
    "        # is a standard neural network dense layer, i.e., y = tanh(W_s • v + b_s). Again,\n",
    "        # the paper does not include bias units.\n",
    "        q_s = F.tanh(self.query_sem(q_k))\n",
    "        q_s = q_s.resize(L)\n",
    "        # # The document equivalent of the above query model for positive document\n",
    "        pos = pos.transpose(1,2)\n",
    "        pos_c = F.tanh(self.doc_conv(pos))\n",
    "        pos_k = kmax_pooling(pos_c, 2, 1)\n",
    "        pos_k = pos_k.transpose(1,2)\n",
    "        pos_s = F.tanh(self.doc_sem(pos_k))\n",
    "        pos_s = pos_s.resize(L)\n",
    "       \n",
    "        dots = q_s.concat(pos_s)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CDSSM()\n",
    "\n",
    "# Build a random data set.\n",
    "import numpy as np\n",
    "sample_size = 10\n",
    "l_Qs = []\n",
    "pos_l_Ds = []\n",
    "\n",
    "(query_len, doc_len) = (5, 100)\n",
    "\n",
    "for i in range(sample_size):\n",
    "    query_len = np.random.randint(1, 10)\n",
    "    l_Q = np.random.rand(1, query_len, WORD_DEPTH)\n",
    "    l_Qs.append(l_Q)\n",
    "    \n",
    "    doc_len = np.random.randint(50, 500)\n",
    "    l_D = np.random.rand(1, doc_len, WORD_DEPTH)\n",
    "    pos_l_Ds.append(l_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 90000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_Qs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 90000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_Qs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
